2:I[2972,["972","static/chunks/972-b2019802b076e131.js","878","static/chunks/878-74de769e6c4fae50.js","534","static/chunks/app/work/page-8d5db3a749ae2d20.js"],""]
3:I[5878,["972","static/chunks/972-b2019802b076e131.js","878","static/chunks/878-74de769e6c4fae50.js","534","static/chunks/app/work/page-8d5db3a749ae2d20.js"],"Image"]
4:I[4707,[],""]
5:I[6423,[],""]
6:I[2798,["972","static/chunks/972-b2019802b076e131.js","373","static/chunks/373-c9d94647dd4aeee5.js","876","static/chunks/876-e00645a8fdf64d5b.js","185","static/chunks/app/layout-0ed60b5196d7bd4b.js"],"ThemeProvider"]
7:I[6088,["972","static/chunks/972-b2019802b076e131.js","373","static/chunks/373-c9d94647dd4aeee5.js","876","static/chunks/876-e00645a8fdf64d5b.js","185","static/chunks/app/layout-0ed60b5196d7bd4b.js"],"Navigation"]
8:I[3284,["972","static/chunks/972-b2019802b076e131.js","373","static/chunks/373-c9d94647dd4aeee5.js","876","static/chunks/876-e00645a8fdf64d5b.js","185","static/chunks/app/layout-0ed60b5196d7bd4b.js"],"Footer"]
0:["FSejKlqBp5MH3kk9BZgJl",[[["",{"children":["work",{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",{"children":["work",{"children":["__PAGE__",{},[["$L1",["$","div",null,{"className":"pt-32 pb-16","children":["$","div",null,{"className":"container max-w-4xl space-y-16","children":[["$","section",null,{"className":"space-y-8","children":[["$","h1",null,{"className":"text-4xl font-bold gradient-text","children":"Aya's Projects"}],["$","p",null,{"className":"text-lg text-muted-foreground","children":"AI / Machine Mearning / Computational Neuroscience"}]]}],["$","section",null,{"className":"space-y-16","children":[["$","$L2","0",{"href":"https://aya0221.github.io/thesis-pdf/thesis_ayaoshima.pdf","target":"_blank","rel":"noopener noreferrer","className":"group p-6 rounded-lg border bg-card hover-lift colorful-border flex flex-col md:flex-row gap-4","children":[["$","div",null,{"className":"flex-1","children":[["$","div",null,{"className":"flex justify-between items-start mb-4","children":[["$","h3",null,{"className":"text-xl font-semibold","children":"Dimensional Reduction for Neuron Data"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-file-text w-5 h-5 text-muted-foreground group-hover:text-primary transition-colors","children":[["$","path","1rqfz7",{"d":"M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"}],["$","path","tnqrlb",{"d":"M14 2v4a2 2 0 0 0 2 2h4"}],["$","path","b1mrlr",{"d":"M10 9H8"}],["$","path","t4e002",{"d":"M16 13H8"}],["$","path","z1uh3a",{"d":"M16 17H8"}],"$undefined"]}]]}],["$","p",null,{"className":"text-muted-foreground mb-4","children":"Explored dimensional reduction techniques (PCA, TCA) applied to large-scale neural datasets. Conducted analysis on complex neural patterns, providing insights into brain activity and computational neuroscience."}],["$","div",null,{"className":"flex flex-wrap gap-2","children":[["$","span","0",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"Statistical Modeling"}],["$","span","1",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"PCA"}],["$","span","2",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"TCA"}],["$","span","3",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"Computational Neuroscience"}],["$","span","4",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"Data Analysis"}],["$","span","5",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"Dimension Reduction"}],["$","span","6",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"Matlab"}],["$","span","7",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"Python"}]]}]]}],["$","div",null,{"className":"flex flex-col flex-1 gap-4","children":[["$","$L3","0",{"src":"/images/thesis_demo.jpg","alt":"Dimensional Reduction for Neuron Data Image 1","width":800,"height":600,"className":"w-full h-auto rounded-lg shadow-md"}],["$","$L3","1",{"src":"/images/thesis_demo_paper1.jpg","alt":"Dimensional Reduction for Neuron Data Image 2","width":800,"height":600,"className":"w-full h-auto rounded-lg shadow-md"}]]}]]}],["$","$L2","1",{"href":"https://github.com/aya0221/Neural-Symbolic-VQA-Sort-of-CLEVR","target":"_blank","rel":"noopener noreferrer","className":"group p-6 rounded-lg border bg-card hover-lift colorful-border flex flex-col md:flex-row gap-4","children":[["$","div",null,{"className":"flex-1","children":[["$","div",null,{"className":"flex justify-between items-start mb-4","children":[["$","h3",null,{"className":"text-xl font-semibold","children":"Neural-Symbolic VQA"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-arrow-up-right w-5 h-5 text-muted-foreground group-hover:text-primary transition-colors","children":[["$","path","1tivn9",{"d":"M7 7h10v10"}],["$","path","1vkiza",{"d":"M7 17 17 7"}],"$undefined"]}]]}],["$","p",null,{"className":"text-muted-foreground mb-4","children":"Designed a multi-modal AI system integrating computer vision, NLP, and symbolic reasoning to enhance visual question answering on the Sort-of-CLEVR dataset. Achieved high accuracy in both relational and non-relational questions."}],["$","div",null,{"className":"flex flex-wrap gap-2","children":[["$","span","0",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"Multi-Modal"}],["$","span","1",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"Computer Vision"}],["$","span","2",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"CNN"}],["$","span","3",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"NLP"}],["$","span","4",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"PyTorch"}],["$","span","5",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"Symbolic Reasoning"}],["$","span","6",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"Python"}]]}]]}],["$","div",null,{"className":"flex flex-col flex-1 gap-4","children":[["$","$L3","0",{"src":"/images/vqa_demophoto.jpg","alt":"Neural-Symbolic VQA Image 1","width":800,"height":600,"className":"w-full h-auto rounded-lg shadow-md"}],["$","$L3","1",{"src":"/images/vqa_techphoto.jpg","alt":"Neural-Symbolic VQA Image 2","width":800,"height":600,"className":"w-full h-auto rounded-lg shadow-md"}]]}]]}],["$","$L2","2",{"href":"https://github.com/aya0221/Object-Tracking-Under-Occlusions","target":"_blank","rel":"noopener noreferrer","className":"group p-6 rounded-lg border bg-card hover-lift colorful-border flex flex-col md:flex-row gap-4","children":[["$","div",null,{"className":"flex-1","children":[["$","div",null,{"className":"flex justify-between items-start mb-4","children":[["$","h3",null,{"className":"text-xl font-semibold","children":"Object Tracking Under Occlusions"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-arrow-up-right w-5 h-5 text-muted-foreground group-hover:text-primary transition-colors","children":[["$","path","1tivn9",{"d":"M7 7h10v10"}],["$","path","1vkiza",{"d":"M7 17 17 7"}],"$undefined"]}]]}],["$","p",null,{"className":"text-muted-foreground mb-4","children":"Built a robust object tracking system combining CNN-based detection and Kalman filters to predict and track occluded objects effectively. Developed using ResNet for feature extraction, enabling accurate tracking under challenging conditions."}],["$","div",null,{"className":"flex flex-wrap gap-2","children":[["$","span","0",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"Mathematical Modeling"}],["$","span","1",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"Kalman Filter"}],["$","span","2",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"Object Tracking"}],["$","span","3",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"CNN"}],["$","span","4",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"ResNet"}],["$","span","5",{"className":"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground","children":"Python"}]]}]]}],["$","div",null,{"className":"flex flex-col flex-1 gap-4","children":[["$","$L3","0",{"src":"/images/ballpic.jpg","alt":"Object Tracking Under Occlusions Image 1","width":800,"height":600,"className":"w-full h-auto rounded-lg shadow-md"}],["$","$L3","1",{"src":"/images/objDetection_ballpic.jpg","alt":"Object Tracking Under Occlusions Image 2","width":800,"height":600,"className":"w-full h-auto rounded-lg shadow-md"}]]}]]}]]}]]}]}],null],null],null]},[null,["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","work","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/1d8967eb8bcb845c.css","precedence":"next","crossOrigin":"$undefined"}]],["$","html",null,{"lang":"en","suppressHydrationWarning":true,"children":["$","body",null,{"className":"__variable_0aa4ae font-sans","children":["$","$L6",null,{"attribute":"class","defaultTheme":"system","enableSystem":true,"disableTransitionOnChange":true,"children":["$","div",null,{"className":"min-h-screen flex flex-col","children":[["$","$L7",null,{}],["$","main",null,{"className":"flex-1 w-full max-w-4xl mx-auto px-4","children":["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[]}]}],["$","$L8",null,{}]," "]}]}]}]}]],null],null],["$L9",null]]]]
9:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Aya Oshima - AI Engineer"}],["$","meta","3",{"name":"description","content":"AI Engineer specializing in ML, Robotics, and Full-stack Development"}],["$","meta","4",{"name":"next-size-adjust"}]]
1:null
