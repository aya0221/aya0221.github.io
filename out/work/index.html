<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/2d141e1a38819612-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/41e07592a33747ee.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-a78e9c7078e128e5.js"/><script src="/_next/static/chunks/fd9d1056-b88f8a4cc02eef8b.js" async=""></script><script src="/_next/static/chunks/117-939434cc6af0b65f.js" async=""></script><script src="/_next/static/chunks/main-app-25d99e694b9cc842.js" async=""></script><script src="/_next/static/chunks/878-4218b8fc1050f006.js" async=""></script><script src="/_next/static/chunks/app/work/page-e076b5abd246d4b7.js" async=""></script><script src="/_next/static/chunks/972-edd2ee9795e590ec.js" async=""></script><script src="/_next/static/chunks/598-ebaef4f280efd328.js" async=""></script><script src="/_next/static/chunks/app/layout-eb753ae0993d2b16.js" async=""></script><script src="/_next/static/chunks/app/page-d945f7a24654736b.js" async=""></script><title>Aya Oshima - AI Engineer</title><meta name="description" content="AI Engineer specializing in ML, Robotics, and Full-stack Development"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__variable_0aa4ae font-sans"><div class="min-h-screen flex flex-col"><header class="fixed top-0 left-0 right-0 z-50 bg-background/80 backdrop-blur-sm"><div class="container flex items-center justify-between h-20"><a class="logo-container text-6xl font-bold" href="/"><span class="logo-static">AYAOSHIMA.</span><span class="logo-extension logo-extension-m text-primary">M</span><span class="logo-extension logo-extension-py text-primary">PY</span><span class="logo-extension logo-extension-cpp text-primary">CPP</span></a><nav class="flex items-center gap-8"><a class="nav-link text-2xl font-bold text-muted-foreground hover:text-primary" href="/about/">about</a><a class="nav-link text-2xl font-bold text-muted-foreground hover:text-primary" href="/work/">work</a><a class="nav-link text-2xl font-bold text-muted-foreground hover:text-primary" href="/blog/">blog</a><a class="nav-link text-2xl font-bold text-muted-foreground hover:text-primary" href="/video/">video</a><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background hover:bg-accent hover:text-accent-foreground h-10 w-10" type="button" id="radix-:Rkcq:" aria-haspopup="menu" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-sun h-[1.2rem] w-[1.2rem] rotate-0 scale-100 transition-all dark:-rotate-90 dark:scale-0"><circle cx="12" cy="12" r="4"></circle><path d="M12 2v2"></path><path d="M12 20v2"></path><path d="m4.93 4.93 1.41 1.41"></path><path d="m17.66 17.66 1.41 1.41"></path><path d="M2 12h2"></path><path d="M20 12h2"></path><path d="m6.34 17.66-1.41 1.41"></path><path d="m19.07 4.93-1.41 1.41"></path></svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-moon absolute h-[1.2rem] w-[1.2rem] rotate-90 scale-0 transition-all dark:rotate-0 dark:scale-100"><path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z"></path></svg><span class="sr-only">Toggle theme</span></button></nav></div></header><main class="flex-1 w-full max-w-4xl mx-auto px-4"><div class="pt-32 pb-16"><div class="container max-w-4xl space-y-16"><section class="space-y-8"><h1 class="text-4xl font-bold gradient-text">Selected Work</h1><p class="text-lg text-muted-foreground">A collection of projects that showcase my expertise in AI, machine learning, and computational neuroscience.</p></section><section class="space-y-16"><div class="p-6 rounded-lg border bg-card hover-lift colorful-border flex flex-col md:flex-row gap-4"><div class="flex-1"><div class="flex justify-between items-start mb-4"><h3 class="text-xl font-semibold">Dimensional Reduction for Neuron Data</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text w-5 h-5 text-muted-foreground group-hover:text-primary transition-colors"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path></svg></div><p class="text-muted-foreground mb-4">Explored dimensional reduction techniques (PCA, TCA) applied to large-scale neural datasets. Conducted analysis on complex neural patterns, providing insights into brain activity and computational neuroscience.</p><div class="flex flex-wrap gap-2"><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">PCA</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">TCA</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Computational Neuroscience</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Data Analysis</span></div></div><div class="flex flex-col flex-1 gap-4"><img alt="Dimensional Reduction for Neuron Data Image 1" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-full h-auto rounded-lg shadow-md" style="color:transparent" src="/images/thesis_demo.jpg"/><img alt="Dimensional Reduction for Neuron Data Image 2" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-full h-auto rounded-lg shadow-md" style="color:transparent" src="/images/thesis_demo_paper1.jpg"/></div></div><div class="p-6 rounded-lg border bg-card hover-lift colorful-border flex flex-col md:flex-row gap-4"><div class="flex-1"><div class="flex justify-between items-start mb-4"><h3 class="text-xl font-semibold">Neural-Symbolic VQA</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-up-right w-5 h-5 text-muted-foreground group-hover:text-primary transition-colors"><path d="M7 7h10v10"></path><path d="M7 17 17 7"></path></svg></div><p class="text-muted-foreground mb-4">Designed a multi-modal AI system integrating computer vision, NLP, and symbolic reasoning to enhance visual question answering on the Sort-of-CLEVR dataset. Achieved high accuracy in both relational and non-relational questions.</p><div class="flex flex-wrap gap-2"><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">CNN</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">NLP</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">PyTorch</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Symbolic Reasoning</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Computer Vision</span></div></div><div class="flex flex-col flex-1 gap-4"><img alt="Neural-Symbolic VQA Image 1" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-full h-auto rounded-lg shadow-md" style="color:transparent" src="/images/vqa_demophoto.jpg"/><img alt="Neural-Symbolic VQA Image 2" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-full h-auto rounded-lg shadow-md" style="color:transparent" src="/images/vqa_techphoto.jpg"/></div></div><div class="p-6 rounded-lg border bg-card hover-lift colorful-border flex flex-col md:flex-row gap-4"><div class="flex-1"><div class="flex justify-between items-start mb-4"><h3 class="text-xl font-semibold">Object Tracking Under Occlusions</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-up-right w-5 h-5 text-muted-foreground group-hover:text-primary transition-colors"><path d="M7 7h10v10"></path><path d="M7 17 17 7"></path></svg></div><p class="text-muted-foreground mb-4">Built a robust object tracking system combining CNN-based detection and Kalman filters to predict and track occluded objects effectively. Developed using ResNet for feature extraction, enabling accurate tracking under challenging conditions.</p><div class="flex flex-wrap gap-2"><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">CNN</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Kalman Filter (Mathematical Modeling)</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Object Tracking</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">ResNet</span></div></div><div class="flex flex-col flex-1 gap-4"><img alt="Object Tracking Under Occlusions Image 1" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-full h-auto rounded-lg shadow-md" style="color:transparent" src="/images/ballpic.jpg"/><img alt="Object Tracking Under Occlusions Image 2" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-full h-auto rounded-lg shadow-md" style="color:transparent" src="/images/objDetection_ballpic.jpg"/></div></div></section></div></div></main><footer class="py-8"><div class="container flex flex-col items-center space-y-8"><div class="flex items-center space-x-10"><a class="bg-green-500 text-white rounded-full p-6 hover:bg-green-600 transition-transform transform hover:scale-125 shadow-lg" href="mailto:ayaoshima.us@gmail.com"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-mail h-10 w-10"><rect width="20" height="16" x="2" y="4" rx="2"></rect><path d="m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"></path></svg><span class="sr-only">Email</span></a><a class="bg-blue-500 text-white rounded-full p-6 hover:bg-blue-600 transition-transform transform hover:scale-125 shadow-lg" href="https://github.com/aya0221"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-10 w-10"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg><span class="sr-only">GitHub</span></a><a class="bg-green-500 text-white rounded-full p-6 hover:bg-green-600 transition-transform transform hover:scale-125 shadow-lg" href="https://www.linkedin.com/in/ayaoshima"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin h-10 w-10"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect width="4" height="12" x="2" y="9"></rect><circle cx="4" cy="4" r="2"></circle></svg><span class="sr-only">LinkedIn</span></a></div><p class="text-green-500 font-bold text-sm">COPYRIGHT AYAOSHIMA 2024</p></div></footer> </div><script src="/_next/static/chunks/webpack-a78e9c7078e128e5.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/2d141e1a38819612-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/41e07592a33747ee.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[2846,[],\"\"]\n5:I[5878,[\"878\",\"static/chunks/878-4218b8fc1050f006.js\",\"534\",\"static/chunks/app/work/page-e076b5abd246d4b7.js\"],\"Image\"]\n6:I[4707,[],\"\"]\n7:I[6423,[],\"\"]\n8:I[8599,[\"972\",\"static/chunks/972-edd2ee9795e590ec.js\",\"598\",\"static/chunks/598-ebaef4f280efd328.js\",\"185\",\"static/chunks/app/layout-eb753ae0993d2b16.js\"],\"Navigation\"]\n9:I[2972,[\"972\",\"static/chunks/972-edd2ee9795e590ec.js\",\"931\",\"static/chunks/app/page-d945f7a24654736b.js\"],\"\"]\nb:I[1060,[],\"\"]\nc:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L3\",null,{\"buildId\":\"uOXOE3E1eJgAh3FS7zC1a\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"work\",\"\"],\"initialTree\":[\"\",{\"children\":[\"work\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"work\",{\"children\":[\"__PAGE__\",{},[[\"$L4\",[\"$\",\"div\",null,{\"className\":\"pt-32 pb-16\",\"children\":[\"$\",\"div\",null,{\"className\":\"container max-w-4xl space-y-16\",\"children\":[[\"$\",\"section\",null,{\"className\":\"space-y-8\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold gradient-text\",\"children\":\"Selected Work\"}],[\"$\",\"p\",null,{\"className\":\"text-lg text-muted-foreground\",\"children\":\"A collection of projects that showcase my expertise in AI, machine learning, and computational neuroscience.\"}]]}],[\"$\",\"section\",null,{\"className\":\"space-y-16\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"p-6 rounded-lg border bg-card hover-lift colorful-border flex flex-col md:flex-row gap-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex justify-between items-start mb-4\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold\",\"children\":\"Dimensional Reduction for Neuron Data\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-file-text w-5 h-5 text-muted-foreground group-hover:text-primary transition-colors\",\"children\":[[\"$\",\"path\",\"1rqfz7\",{\"d\":\"M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z\"}],[\"$\",\"path\",\"tnqrlb\",{\"d\":\"M14 2v4a2 2 0 0 0 2 2h4\"}],[\"$\",\"path\",\"b1mrlr\",{\"d\":\"M10 9H8\"}],[\"$\",\"path\",\"t4e002\",{\"d\":\"M16 13H8\"}],[\"$\",\"path\",\"z1uh3a\",{\"d\":\"M16 17H8\"}],\"$undefined\"]}]]}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground mb-4\",\"children\":\"Explored dimensional reduction techniques (PCA, TCA) applied to large-scale neural datasets. Conducted analysis on complex neural patterns, providing insights into brain activity and computational neuroscience.\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"PCA\"}],[\"$\",\"span\",\"1\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"TCA\"}],[\"$\",\"span\",\"2\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Computational Neuroscience\"}],[\"$\",\"span\",\"3\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Data Analysis\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col flex-1 gap-4\",\"children\":[[\"$\",\"$L5\",\"0\",{\"src\":\"/images/thesis_demo.jpg\",\"alt\":\"Dimensional Reduction for Neuron Data Image 1\",\"width\":800,\"height\":600,\"className\":\"w-full h-auto rounded-lg shadow-md\"}],[\"$\",\"$L5\",\"1\",{\"src\":\"/images/thesis_demo_paper1.jpg\",\"alt\":\"Dimensional Reduction for Neuron Data Image 2\",\"width\":800,\"height\":600,\"className\":\"w-full h-auto rounded-lg shadow-md\"}]]}]]}],[\"$\",\"div\",\"1\",{\"className\":\"p-6 rounded-lg border bg-card hover-lift colorful-border flex flex-col md:flex-row gap-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex justify-between items-start mb-4\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold\",\"children\":\"Neural-Symbolic VQA\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-arrow-up-right w-5 h-5 text-muted-foreground group-hover:text-primary transition-colors\",\"children\":[[\"$\",\"path\",\"1tivn9\",{\"d\":\"M7 7h10v10\"}],[\"$\",\"path\",\"1vkiza\",{\"d\":\"M7 17 17 7\"}],\"$undefined\"]}]]}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground mb-4\",\"children\":\"Designed a multi-modal AI system integrating computer vision, NLP, and symbolic reasoning to enhance visual question answering on the Sort-of-CLEVR dataset. Achieved high accuracy in both relational and non-relational questions.\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"CNN\"}],[\"$\",\"span\",\"1\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"NLP\"}],[\"$\",\"span\",\"2\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"PyTorch\"}],[\"$\",\"span\",\"3\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Symbolic Reasoning\"}],[\"$\",\"span\",\"4\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Computer Vision\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col flex-1 gap-4\",\"children\":[[\"$\",\"$L5\",\"0\",{\"src\":\"/images/vqa_demophoto.jpg\",\"alt\":\"Neural-Symbolic VQA Image 1\",\"width\":800,\"height\":600,\"className\":\"w-full h-auto rounded-lg shadow-md\"}],[\"$\",\"$L5\",\"1\",{\"src\":\"/images/vqa_techphoto.jpg\",\"alt\":\"Neural-Symbolic VQA Image 2\",\"width\":800,\"height\":600,\"className\":\"w-full h-auto rounded-lg shadow-md\"}]]}]]}],[\"$\",\"div\",\"2\",{\"className\":\"p-6 rounded-lg border bg-card hover-lift colorful-border flex flex-col md:flex-row gap-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex justify-between items-start mb-4\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold\",\"children\":\"Object Tracking Under Occlusions\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-arrow-up-right w-5 h-5 text-muted-foreground group-hover:text-primary transition-colors\",\"children\":[[\"$\",\"path\",\"1tivn9\",{\"d\":\"M7 7h10v10\"}],[\"$\",\"path\",\"1vkiza\",{\"d\":\"M7 17 17 7\"}],\"$undefined\"]}]]}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground mb-4\",\"children\":\"Built a robust object tracking system combining CNN-based detection and Kalman filters to predict and track occluded objects effectively. Developed using ResNet for feature extraction, enabling accurate tracking under challenging conditions.\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"CNN\"}],[\"$\",\"span\",\"1\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Kalman Filter (Mathematical Modeling)\"}],[\"$\",\"span\",\"2\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Object Tracking\"}],[\"$\",\"span\",\"3\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"ResNet\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col flex-1 gap-4\",\"children\":[[\"$\",\"$L5\",\"0\",{\"src\":\"/images/ballpic.jpg\",\"alt\":\"Object Tracking Under Occlusions Image 1\",\"width\":800,\"height\":600,\"className\":\"w-full h-auto rounded-lg shadow-md\"}],[\"$\",\"$L5\",\"1\",{\"src\":\"/images/objDetection_ballpic.jpg\",\"alt\":\"Object Tracking Under Occlusions Image 2\",\"width\":800,\"height\":600,\"className\":\"w-full h-auto rounded-lg shadow-md\"}]]}]]}]]}]]}]}],null],null],null]},[null,[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"work\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/41e07592a33747ee.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"__variable_0aa4ae font-sans\",\"children\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex flex-col\",\"children\":[[\"$\",\"$L8\",null,{}],[\"$\",\"main\",null,{\"className\":\"flex-1 w-full max-w-4xl mx-auto px-4\",\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}],[\"$\",\"footer\",null,{\"className\":\"py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"container flex flex-col items-center space-y-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-10\",\"children\":[[\"$\",\"$L9\",null,{\"href\":\"mailto:ayaoshima.us@gmail.com\",\"className\":\"bg-green-500 text-white rounded-full p-6 hover:bg-green-600 transition-transform transform hover:scale-125 shadow-lg\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-mail h-10 w-10\",\"children\":[[\"$\",\"rect\",\"18n3k1\",{\"width\":\"20\",\"height\":\"16\",\"x\":\"2\",\"y\":\"4\",\"rx\":\"2\"}],[\"$\",\"path\",\"1ocrg3\",{\"d\":\"m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"Email\"}]]}],[\"$\",\"$L9\",null,{\"href\":\"https://github.com/aya0221\",\"className\":\"bg-blue-500 text-white rounded-full p-6 hover:bg-blue-600 transition-transform transform hover:scale-125 shadow-lg\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-github h-10 w-10\",\"children\":[[\"$\",\"path\",\"tonef\",{\"d\":\"M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4\"}],[\"$\",\"path\",\"9comsn\",{\"d\":\"M9 18c-4.51 2-5-2-7-2\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"GitHub\"}]]}],[\"$\",\"$L9\",null,{\"href\":\"https://www.linkedin.com/in/ayaoshima\",\"className\":\"bg-green-500 text-white rounded-full p-6 hover:bg-green-600 transition-transform transform hover:scale-125 shadow-lg\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-linkedin h-10 w-10\",\"children\":[[\"$\",\"path\",\"c2jq9f\",{\"d\":\"M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z\"}],[\"$\",\"rect\",\"mk3on5\",{\"width\":\"4\",\"height\":\"12\",\"x\":\"2\",\"y\":\"9\"}],[\"$\",\"circle\",\"bt5ra8\",{\"cx\":\"4\",\"cy\":\"4\",\"r\":\"2\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"LinkedIn\"}]]}]]}],[\"$\",\"p\",null,{\"className\":\"text-green-500 font-bold text-sm\",\"children\":\"COPYRIGHT AYAOSHIMA 2024\"}]]}]}],\" \"]}]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$La\"],\"globalErrorComponent\":\"$b\",\"missingSlots\":\"$Wc\"}]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Aya Oshima - AI Engineer\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"AI Engineer specializing in ML, Robotics, and Full-stack Development\"}],[\"$\",\"meta\",\"4\",{\"name\":\"next-size-adjust\"}]]\n4:null\n"])</script></body></html>