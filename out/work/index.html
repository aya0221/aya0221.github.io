<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/2d141e1a38819612-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/1d8967eb8bcb845c.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-a78e9c7078e128e5.js"/><script src="/_next/static/chunks/fd9d1056-3750f1c68fb99acb.js" async=""></script><script src="/_next/static/chunks/117-509dfa0ee4391f99.js" async=""></script><script src="/_next/static/chunks/main-app-25d99e694b9cc842.js" async=""></script><script src="/_next/static/chunks/972-b2019802b076e131.js" async=""></script><script src="/_next/static/chunks/878-74de769e6c4fae50.js" async=""></script><script src="/_next/static/chunks/app/work/page-8d5db3a749ae2d20.js" async=""></script><script src="/_next/static/chunks/373-c9d94647dd4aeee5.js" async=""></script><script src="/_next/static/chunks/876-e00645a8fdf64d5b.js" async=""></script><script src="/_next/static/chunks/app/layout-0ed60b5196d7bd4b.js" async=""></script><title>Aya Oshima - AI Engineer</title><meta name="description" content="AI Engineer specializing in ML, Robotics, and Full-stack Development"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__variable_0aa4ae font-sans"><script>((e,t,r,n,o,a,i,l)=>{let u=document.documentElement,s=["light","dark"];function c(t){(Array.isArray(e)?e:[e]).forEach(e=>{let r="class"===e,n=r&&a?o.map(e=>a[e]||e):o;r?(u.classList.remove(...n),u.classList.add(t)):u.setAttribute(e,t)}),l&&s.includes(t)&&(u.style.colorScheme=t)}if(n)c(n);else try{let e=localStorage.getItem(t)||r,n=i&&"system"===e?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":e;c(n)}catch(e){}})("class","theme","system",null,["light","dark"],null,true,true)</script><div class="min-h-screen flex flex-col"><header class="fixed top-0 left-0 right-0 z-50 bg-background/80 backdrop-blur-sm"><div class="container flex items-center justify-between h-20 px-4"><a class="logo-container text-4xl sm:text-5xl md:text-6xl font-bold" href="/"><span class="logo-static">AYAOSHIMA.</span><span class="logo-extension logo-extension-m text-primary">M</span><span class="logo-extension logo-extension-py text-primary">PY</span><span class="logo-extension logo-extension-cpp text-primary">CPP</span></a><button class="md:hidden"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-menu w-6 h-6"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button><nav class="hidden md:flex items-center gap-4 lg:gap-8"><a class="nav-link text-xl lg:text-2xl font-bold text-muted-foreground hover:text-primary" href="/about/">about</a><a class="nav-link text-xl lg:text-2xl font-bold text-muted-foreground hover:text-primary" href="/work/">work</a><a class="nav-link text-xl lg:text-2xl font-bold text-muted-foreground hover:text-primary" href="/blog/">blog</a><a class="nav-link text-xl lg:text-2xl font-bold text-muted-foreground hover:text-primary" href="/video/">video</a><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background hover:bg-accent hover:text-accent-foreground h-10 w-10 text-lg" type="button" id="radix-:R4pkq:" aria-haspopup="menu" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-sun h-[1.2rem] w-[1.2rem] rotate-0 scale-100 transition-all dark:-rotate-90 dark:scale-0"><circle cx="12" cy="12" r="4"></circle><path d="M12 2v2"></path><path d="M12 20v2"></path><path d="m4.93 4.93 1.41 1.41"></path><path d="m17.66 17.66 1.41 1.41"></path><path d="M2 12h2"></path><path d="M20 12h2"></path><path d="m6.34 17.66-1.41 1.41"></path><path d="m19.07 4.93-1.41 1.41"></path></svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-moon absolute h-[1.2rem] w-[1.2rem] rotate-90 scale-0 transition-all dark:rotate-0 dark:scale-100"><path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z"></path></svg><span class="sr-only">Toggle theme</span></button></nav></div></header><main class="flex-1 w-full max-w-4xl mx-auto px-4"><div class="pt-32 pb-16"><div class="container max-w-4xl space-y-16"><section class="space-y-8"><h1 class="text-4xl font-bold gradient-text">Aya&#x27;s Projects</h1><p class="text-lg text-muted-foreground">AI / Machine Mearning / Computational Neuroscience</p></section><section class="space-y-16"><a target="_blank" rel="noopener noreferrer" class="group p-6 rounded-lg border bg-card hover-lift colorful-border flex flex-col md:flex-row gap-4" href="https://aya0221.github.io/thesis-pdf/thesis_ayaoshima.pdf"><div class="flex-1"><div class="flex justify-between items-start mb-4"><h3 class="text-xl font-semibold">Dimensional Reduction for Neuron Data</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text w-5 h-5 text-muted-foreground group-hover:text-primary transition-colors"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path></svg></div><p class="text-muted-foreground mb-4">Explored dimensional reduction techniques (PCA, TCA) applied to large-scale neural datasets. Conducted analysis on complex neural patterns, providing insights into brain activity and computational neuroscience.</p><div class="flex flex-wrap gap-2"><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Statistical Modeling</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">PCA</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">TCA</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Computational Neuroscience</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Data Analysis</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Dimension Reduction</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Matlab</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Python</span></div></div><div class="flex flex-col flex-1 gap-4"><img alt="Dimensional Reduction for Neuron Data Image 1" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-full h-auto rounded-lg shadow-md" style="color:transparent" src="/images/thesis_demo.jpg"/><img alt="Dimensional Reduction for Neuron Data Image 2" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-full h-auto rounded-lg shadow-md" style="color:transparent" src="/images/thesis_demo_paper1.jpg"/></div></a><a target="_blank" rel="noopener noreferrer" class="group p-6 rounded-lg border bg-card hover-lift colorful-border flex flex-col md:flex-row gap-4" href="https://github.com/aya0221/Neural-Symbolic-VQA-Sort-of-CLEVR"><div class="flex-1"><div class="flex justify-between items-start mb-4"><h3 class="text-xl font-semibold">Neural-Symbolic VQA</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-up-right w-5 h-5 text-muted-foreground group-hover:text-primary transition-colors"><path d="M7 7h10v10"></path><path d="M7 17 17 7"></path></svg></div><p class="text-muted-foreground mb-4">Designed a multi-modal AI system integrating computer vision, NLP, and symbolic reasoning to enhance visual question answering on the Sort-of-CLEVR dataset. Achieved high accuracy in both relational and non-relational questions.</p><div class="flex flex-wrap gap-2"><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Multi-Modal</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Computer Vision</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">CNN</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">NLP</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">PyTorch</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Symbolic Reasoning</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Python</span></div></div><div class="flex flex-col flex-1 gap-4"><img alt="Neural-Symbolic VQA Image 1" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-full h-auto rounded-lg shadow-md" style="color:transparent" src="/images/vqa_demophoto.jpg"/><img alt="Neural-Symbolic VQA Image 2" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-full h-auto rounded-lg shadow-md" style="color:transparent" src="/images/vqa_techphoto.jpg"/></div></a><a target="_blank" rel="noopener noreferrer" class="group p-6 rounded-lg border bg-card hover-lift colorful-border flex flex-col md:flex-row gap-4" href="https://github.com/aya0221/Object-Tracking-Under-Occlusions"><div class="flex-1"><div class="flex justify-between items-start mb-4"><h3 class="text-xl font-semibold">Object Tracking Under Occlusions</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-up-right w-5 h-5 text-muted-foreground group-hover:text-primary transition-colors"><path d="M7 7h10v10"></path><path d="M7 17 17 7"></path></svg></div><p class="text-muted-foreground mb-4">Built a robust object tracking system combining CNN-based detection and Kalman filters to predict and track occluded objects effectively. Developed using ResNet for feature extraction, enabling accurate tracking under challenging conditions.</p><div class="flex flex-wrap gap-2"><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Mathematical Modeling</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Kalman Filter</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Object Tracking</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">CNN</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">ResNet</span><span class="text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground">Python</span></div></div><div class="flex flex-col flex-1 gap-4"><img alt="Object Tracking Under Occlusions Image 1" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-full h-auto rounded-lg shadow-md" style="color:transparent" src="/images/ballpic.jpg"/><img alt="Object Tracking Under Occlusions Image 2" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-full h-auto rounded-lg shadow-md" style="color:transparent" src="/images/objDetection_ballpic.jpg"/></div></a></section></div></div></main><footer class="py-8 bg-gray-100 dark:bg-gray-800 transition-colors duration-300"><div class="container flex flex-col items-center space-y-8"><div class="flex items-center space-x-10"><button class="bg-green-600 text-white rounded-full p-6 hover:bg-green-700 transition-transform transform hover:scale-125 shadow-lg flex flex-col items-center" aria-label="Contact via Email" title="Contact via Email"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-mail h-10 w-10"><rect width="20" height="16" x="2" y="4" rx="2"></rect><path d="m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"></path></svg></button><a target="_blank" rel="noopener noreferrer" class="bg-gray-800 text-white rounded-full p-6 hover:bg-gray-900 transition-transform transform hover:scale-125 shadow-lg" aria-label="GitHub" title="GitHub" href="https://github.com/aya0221"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-10 w-10"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a target="_blank" rel="noopener noreferrer" class="bg-blue-700 text-white rounded-full p-6 hover:bg-blue-800 transition-transform transform hover:scale-125 shadow-lg" aria-label="LinkedIn" title="LinkedIn" href="https://www.linkedin.com/in/ayaoshima"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin h-10 w-10"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect width="4" height="12" x="2" y="9"></rect><circle cx="4" cy="4" r="2"></circle></svg></a><button class="bg-purple-500 text-white rounded-full p-6 hover:bg-purple-600 transition-transform transform hover:scale-125 shadow-lg" aria-label="Open Chat" title="Open Chat"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-square h-10 w-10"><path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg></button></div><p class="text-green-500 font-bold text-sm">© AYAOSHIMA 2024</p></div></footer> </div><script src="/_next/static/chunks/webpack-a78e9c7078e128e5.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/2d141e1a38819612-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/1d8967eb8bcb845c.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[2846,[],\"\"]\n5:I[2972,[\"972\",\"static/chunks/972-b2019802b076e131.js\",\"878\",\"static/chunks/878-74de769e6c4fae50.js\",\"534\",\"static/chunks/app/work/page-8d5db3a749ae2d20.js\"],\"\"]\n6:I[5878,[\"972\",\"static/chunks/972-b2019802b076e131.js\",\"878\",\"static/chunks/878-74de769e6c4fae50.js\",\"534\",\"static/chunks/app/work/page-8d5db3a749ae2d20.js\"],\"Image\"]\n7:I[4707,[],\"\"]\n8:I[6423,[],\"\"]\n9:I[2798,[\"972\",\"static/chunks/972-b2019802b076e131.js\",\"373\",\"static/chunks/373-c9d94647dd4aeee5.js\",\"876\",\"static/chunks/876-e00645a8fdf64d5b.js\",\"185\",\"static/chunks/app/layout-0ed60b5196d7bd4b.js\"],\"ThemeProvider\"]\na:I[6088,[\"972\",\"static/chunks/972-b2019802b076e131.js\",\"373\",\"static/chunks/373-c9d94647dd4aeee5.js\",\"876\",\"static/chunks/876-e00645a8fdf64d5b.js\",\"185\",\"static/chunks/app/layout-0ed60b5196d7bd4b.js\"],\"Navigation\"]\nb:I[3284,[\"972\",\"static/chunks/972-b2019802b076e131.js\",\"373\",\"static/chunks/373-c9d94647dd4aeee5.js\",\"876\",\"static/chunks/876-e00645a8fdf64d5b.js\",\"185\",\"static/chunks/app/layout-0ed60b5196d7bd4b.js\"],\"Footer\"]\nd:I[1060,[],\"\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L3\",null,{\"buildId\":\"FSejKlqBp5MH3kk9BZgJl\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"work\",\"\"],\"initialTree\":[\"\",{\"children\":[\"work\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"work\",{\"children\":[\"__PAGE__\",{},[[\"$L4\",[\"$\",\"div\",null,{\"className\":\"pt-32 pb-16\",\"children\":[\"$\",\"div\",null,{\"className\":\"container max-w-4xl space-y-16\",\"children\":[[\"$\",\"section\",null,{\"className\":\"space-y-8\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold gradient-text\",\"children\":\"Aya's Projects\"}],[\"$\",\"p\",null,{\"className\":\"text-lg text-muted-foreground\",\"children\":\"AI / Machine Mearning / Computational Neuroscience\"}]]}],[\"$\",\"section\",null,{\"className\":\"space-y-16\",\"children\":[[\"$\",\"$L5\",\"0\",{\"href\":\"https://aya0221.github.io/thesis-pdf/thesis_ayaoshima.pdf\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"group p-6 rounded-lg border bg-card hover-lift colorful-border flex flex-col md:flex-row gap-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex justify-between items-start mb-4\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold\",\"children\":\"Dimensional Reduction for Neuron Data\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-file-text w-5 h-5 text-muted-foreground group-hover:text-primary transition-colors\",\"children\":[[\"$\",\"path\",\"1rqfz7\",{\"d\":\"M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z\"}],[\"$\",\"path\",\"tnqrlb\",{\"d\":\"M14 2v4a2 2 0 0 0 2 2h4\"}],[\"$\",\"path\",\"b1mrlr\",{\"d\":\"M10 9H8\"}],[\"$\",\"path\",\"t4e002\",{\"d\":\"M16 13H8\"}],[\"$\",\"path\",\"z1uh3a\",{\"d\":\"M16 17H8\"}],\"$undefined\"]}]]}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground mb-4\",\"children\":\"Explored dimensional reduction techniques (PCA, TCA) applied to large-scale neural datasets. Conducted analysis on complex neural patterns, providing insights into brain activity and computational neuroscience.\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Statistical Modeling\"}],[\"$\",\"span\",\"1\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"PCA\"}],[\"$\",\"span\",\"2\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"TCA\"}],[\"$\",\"span\",\"3\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Computational Neuroscience\"}],[\"$\",\"span\",\"4\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Data Analysis\"}],[\"$\",\"span\",\"5\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Dimension Reduction\"}],[\"$\",\"span\",\"6\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Matlab\"}],[\"$\",\"span\",\"7\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Python\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col flex-1 gap-4\",\"children\":[[\"$\",\"$L6\",\"0\",{\"src\":\"/images/thesis_demo.jpg\",\"alt\":\"Dimensional Reduction for Neuron Data Image 1\",\"width\":800,\"height\":600,\"className\":\"w-full h-auto rounded-lg shadow-md\"}],[\"$\",\"$L6\",\"1\",{\"src\":\"/images/thesis_demo_paper1.jpg\",\"alt\":\"Dimensional Reduction for Neuron Data Image 2\",\"width\":800,\"height\":600,\"className\":\"w-full h-auto rounded-lg shadow-md\"}]]}]]}],[\"$\",\"$L5\",\"1\",{\"href\":\"https://github.com/aya0221/Neural-Symbolic-VQA-Sort-of-CLEVR\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"group p-6 rounded-lg border bg-card hover-lift colorful-border flex flex-col md:flex-row gap-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex justify-between items-start mb-4\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold\",\"children\":\"Neural-Symbolic VQA\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-arrow-up-right w-5 h-5 text-muted-foreground group-hover:text-primary transition-colors\",\"children\":[[\"$\",\"path\",\"1tivn9\",{\"d\":\"M7 7h10v10\"}],[\"$\",\"path\",\"1vkiza\",{\"d\":\"M7 17 17 7\"}],\"$undefined\"]}]]}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground mb-4\",\"children\":\"Designed a multi-modal AI system integrating computer vision, NLP, and symbolic reasoning to enhance visual question answering on the Sort-of-CLEVR dataset. Achieved high accuracy in both relational and non-relational questions.\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Multi-Modal\"}],[\"$\",\"span\",\"1\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Computer Vision\"}],[\"$\",\"span\",\"2\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"CNN\"}],[\"$\",\"span\",\"3\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"NLP\"}],[\"$\",\"span\",\"4\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"PyTorch\"}],[\"$\",\"span\",\"5\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Symbolic Reasoning\"}],[\"$\",\"span\",\"6\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Python\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col flex-1 gap-4\",\"children\":[[\"$\",\"$L6\",\"0\",{\"src\":\"/images/vqa_demophoto.jpg\",\"alt\":\"Neural-Symbolic VQA Image 1\",\"width\":800,\"height\":600,\"className\":\"w-full h-auto rounded-lg shadow-md\"}],[\"$\",\"$L6\",\"1\",{\"src\":\"/images/vqa_techphoto.jpg\",\"alt\":\"Neural-Symbolic VQA Image 2\",\"width\":800,\"height\":600,\"className\":\"w-full h-auto rounded-lg shadow-md\"}]]}]]}],[\"$\",\"$L5\",\"2\",{\"href\":\"https://github.com/aya0221/Object-Tracking-Under-Occlusions\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"group p-6 rounded-lg border bg-card hover-lift colorful-border flex flex-col md:flex-row gap-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex justify-between items-start mb-4\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold\",\"children\":\"Object Tracking Under Occlusions\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-arrow-up-right w-5 h-5 text-muted-foreground group-hover:text-primary transition-colors\",\"children\":[[\"$\",\"path\",\"1tivn9\",{\"d\":\"M7 7h10v10\"}],[\"$\",\"path\",\"1vkiza\",{\"d\":\"M7 17 17 7\"}],\"$undefined\"]}]]}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground mb-4\",\"children\":\"Built a robust object tracking system combining CNN-based detection and Kalman filters to predict and track occluded objects effectively. Developed using ResNet for feature extraction, enabling accurate tracking under challenging conditions.\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2\",\"children\":[[\"$\",\"span\",\"0\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Mathematical Modeling\"}],[\"$\",\"span\",\"1\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Kalman Filter\"}],[\"$\",\"span\",\"2\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Object Tracking\"}],[\"$\",\"span\",\"3\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"CNN\"}],[\"$\",\"span\",\"4\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"ResNet\"}],[\"$\",\"span\",\"5\",{\"className\":\"text-sm px-3 py-1 rounded-full bg-muted text-muted-foreground\",\"children\":\"Python\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col flex-1 gap-4\",\"children\":[[\"$\",\"$L6\",\"0\",{\"src\":\"/images/ballpic.jpg\",\"alt\":\"Object Tracking Under Occlusions Image 1\",\"width\":800,\"height\":600,\"className\":\"w-full h-auto rounded-lg shadow-md\"}],[\"$\",\"$L6\",\"1\",{\"src\":\"/images/objDetection_ballpic.jpg\",\"alt\":\"Object Tracking Under Occlusions Image 2\",\"width\":800,\"height\":600,\"className\":\"w-full h-auto rounded-lg shadow-md\"}]]}]]}]]}]]}]}],null],null],null]},[null,[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"work\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/1d8967eb8bcb845c.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"__variable_0aa4ae font-sans\",\"children\":[\"$\",\"$L9\",null,{\"attribute\":\"class\",\"defaultTheme\":\"system\",\"enableSystem\":true,\"disableTransitionOnChange\":true,\"children\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex flex-col\",\"children\":[[\"$\",\"$La\",null,{}],[\"$\",\"main\",null,{\"className\":\"flex-1 w-full max-w-4xl mx-auto px-4\",\"children\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}],[\"$\",\"$Lb\",null,{}],\" \"]}]}]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Aya Oshima - AI Engineer\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"AI Engineer specializing in ML, Robotics, and Full-stack Development\"}],[\"$\",\"meta\",\"4\",{\"name\":\"next-size-adjust\"}]]\n4:null\n"])</script></body></html>